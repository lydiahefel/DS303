---
title: "Qualitative Predictors"
author: 
  - "Lydia Hefel"
  - "DS303, SP25"
  - "Prof. Amber Camp"
date: 2/7/25
format: html
toc: true
editor: visual
theme: spacelab
---

## Qualitative Predictors

So far, we have used the `Boston` data to explore variables that are all numerical/quantitative. In this document, we will examine data that is qualitative in nature.

## Let's try it

Start by loading the `ISLR2` package, and also `lme4` and `tidyverse`. We will be using `Carseats` data from `ISLR2`.

```{r, echo = FALSE, message = FALSE}
library(ISLR2)
library(lme4)
library(tidyverse)
```

## Boston Housing Data

Take a moment to use the `skimr` package to examine the `Boston` data. You are probably quite familiar with this data at this point.

`skimr` will create a new table for you that summarizes the `Boston` data, including details like number of `NA`s, the mean and sd, etc. Look at the different variables listed in the second column, and the associated type listed in the first column. See how every variable is numeric?

```{r}
# install.packages("skimr") # install if needed
skim_boston <- skimr::skim(Boston)
view(skim_boston)
```

## Carseats data

Use `skimr` to inspect the Carseats data. You should see a few variables of type *factor*. Notice how the mean calculation doesn't apply, and there are new columns for factor information in your `skimr` table.

```{r}
carseats <- Carseats # assign, for easy access later
skim_carseats <- skimr::skim(carseats)
view(skim_carseats)
```

### Examine the categorical data

Examine the factor variables. These are qualitative in nature and are evaluated as ***categorical data***.

Qualitative predictors such as `ShelveLoc` consist of different ***levels*** within that category. ***Levels*** refer to the distinct categories or values that a categorical variable can take. For example, levels of the made-up variable `Fruit` could include apple, banana, and grape. In the case of `ShelveLoc`, the levels are bad, medium, and good. I'm glad you're reading this and not just looking at the code! Tell me your favorite fruit for an extra point.

By the way, `ShelveLoc` refers to the shelving location for carseats—that is, the place on a store shelf in which the car seat is displayed.

```{r}
summary(carseats$ShelveLoc) # three levels (bad, good, medium)

summary(carseats$Urban) # two levels (no, yes)

summary(carseats$US) # two levels (no, yes)

head(carseats)
```

### Dummy coding (a.k.a. treatment coding)

Given a qualitative variable such as `ShelveLoc`, `R` generates dummy variables automatically. This is referred to as *dummy coding*. We'll spend more time in a future class discussing coding (including dummy coding). Consider this a soft launch.

We can use `contrasts()` to see how the dummy coding is applied.

```{r}
contrasts(carseats$ShelveLoc)
```

Here is how to interpret that matrix.

-   `R` has created a `ShelveLocGood` dummy variable that takes on a value of 1 if the shelving location is **good**, and 0 otherwise.

-   It has also created a `ShelveLocMedium` dummy variable that equals 1 if the shelving location is **medium**, and 0 otherwise.

-   A **bad** shelving location corresponds to a zero for each of the two dummy variables. **Bad** is the *reference level* (or baseline category), which is why it doesn't get its own dummy variable.

This allows us to include categorical predictors in regression models, which require numeric inputs.

### Run a linear regression model.

Let's run a model to see it in action. We will attempt to predict `Sales` based on `ShelveLoc`.

```{r}
lm.shelf <- lm(Sales ~ ShelveLoc, data = carseats)
summary(lm.shelf)
```

### Interpretation of `lm.shelf`

Note that the output does not just say `ShelveLoc` - - you have `ShelveLocGood` and `ShelveLocMedium`. Where is `ShelveLocBad`? It is not listed because it serves as the **reference category** in the model.

This means that the coefficients for `ShelveLocGood` and `ShelveLocMedium` show how much sales increase **compared to** `ShelveLocBad`. We don’t need a separate coefficient for `ShelveLocBad` because it is the baseline that the other categories are compared against. We see:

-   The coefficient for `ShelveLocGood` is positive, indicating that a Good shelving location is associated with higher sales (relative to a Bad location)

-   `ShelveLocMedium` has a smaller positive coefficient, indicating that a Medium shelving location is associated with higher sales than a bad shelving location, but lower sales than a good shelving location

### Visualization

Scatterplots are not really useful here with the categorical data (try it, you'll see what I mean). Use a boxplot or violin plot instead.

Evaluate the below plots. Do they align with the model interpretation above?

```{r}
ggplot(carseats, aes(x = ShelveLoc, y = Sales)) +
  geom_boxplot() +
  stat_summary(fun = mean, geom = "point", color = "red", size = 2) + # red dot shows mean
  scale_x_discrete(limits = c("Bad", "Medium", "Good"))

ggplot(carseats, aes(x = ShelveLoc, y = Sales)) +
  geom_violin() +
  scale_x_discrete(limits = c("Bad", "Medium", "Good"))
```

To inspect your model, you can also create a residuals vs. fitted values plot to check for homoscedasticity (constant variance of residuals) and a Q-Q plot to check if the residuals are normally distributed.

When we fit a regression model, we're claiming that our predictors explain the systematic variation in the outcome, and what's left over (the residuals) is just random noise. If the residuals have different variances across predictor levels, that suggests there's still systematic pattern we haven't captured - the noise isn't purely random.

*Heteroscedasticity* often hints at:

-   Missing predictors

-   Wrong functional form (maybe you need a transformation or interaction)

-   The relationship changes across the range of X

```{r}
# create a new df with residual and predicted values
predicted <- predict(lm.shelf)
residuals <- resid(lm.shelf)
residuals_data <- data.frame(fitted = predicted, residuals = residuals)

# residuals v. fitted values plot
ggplot(residuals_data, aes(x = fitted, y = residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Residuals vs. Fitted Values",
       x = "Fitted Values",
       y = "Residuals")

# Q-Q plot
ggplot(data.frame(residuals = residuals), aes(sample = residuals)) +
  geom_qq() +
  geom_qq_line() +
  labs(title = "Normal Q-Q Plot of Residuals") +
  theme_minimal()
```

## Your turn

### Predicting Price based on Shelf Location

Below, fit a linear regression model to predict `Price` based on `ShelveLoc`. I'll leave it to you to recall the syntax and build your model.

```{r}
lm.price <- lm(Price ~ ShelveLoc, data = carseats)
summary(lm.price)

#Student Choice
lm.adv <- lm(Advertising ~ ShelveLoc, data = carseats)
summary(lm.adv)

#Class Choice
lm.new <- lm(Population ~ Urban + Price, data = carseats)
summary(lm.new)
```

Create a boxplot or violin plot to aid in your interpretation, and write in your interpretation below.

```{r}
ggplot(carseats, aes(x = ShelveLoc, y = Price)) +
  geom_boxplot() +
  stat_summary(fun = mean, geom = "point", color = "red", size = 2) + # red dot shows mean
  scale_x_discrete(limits = c("Bad", "Medium", "Good"))

#Student Choice
ggplot(carseats, aes(x = ShelveLoc, y = Advertising)) +
  geom_boxplot() +
  stat_summary(fun = mean, geom = "point", color = "red", size = 2) + # red dot shows mean
  scale_x_discrete(limits = c("Bad", "Medium", "Good"))
```

#### Interpretation of `lm.price`

-   `Intercept`: On average, if a car seat is on a Bad shelf (the baseline), the estimated price is \$114.27

-   `ShelveLocGood`: When on a Good shelf, car seats are expected to be associated with higher prices (relative to on a Bad shelf).

-   `ShelveLocMedium`: When on a Medium shelf, car seats are expected to be associated with higher prices than a bad shelving location, but lower prices than a good shelving location.

-   `R-squared`: This model explains 0.26% of variation in price.

-   `F-statistic`: The low F-statistic and a high *p*-value indicates that shelving location does not have a statistically significant impact on the price of a car seat.

### Predict Sales using all variables

Below, fit a linear regression model to predict `Sales` that includes ALL variables as well as a few interaction terms of your choice.

```{r}

lm.carseats <- lm(Sales ~ . + CompPrice:Price + Advertising:Population, data = carseats)
summary(lm.carseats)

lm.price <- lm(Sales ~ Price * ShelveLoc, data = carseats)
summary(lm.price)
```

#### Interpretation of `lm.carseats`

Provide a summary of the model output, including key coefficients and their significance levels. Discuss which variables are statistically significant predictors of `Sales`.

(Write answer here)

#### Practical implications

Discuss the practical implications of your findings. How can this model inform business decisions regarding pricing, advertising, or product placement, etc?

(Write your answer here)

## Bonus: Formatting model output

Using `summary()` is great for viewing model outputs, but sometimes you may want something a bit more visually appealing. Here are a few options:

#### Broom

```{r}
# install.packages("broom")
library(broom)
broom::tidy(lm.price)
```

#### Stargazer

```{r}
# install.packages("stargazer")
library(stargazer)
stargazer(lm.price, type = "text")
```

#### modelsummary

```{r}
# install.packages("modelsummary")
library(modelsummary)
modelsummary(lm.price)
```

#### sjPlot

```{r}
# install.packages("sjPlot")
library(sjPlot)
tab_model(lm.price)
```

#### kableExtra

```{r}
# install.packages("knitr")
# install.packages("kableExtra")
library(knitr)
library(kableExtra)
kable(broom::tidy(lm.price)) %>% 
  kable_styling()
```
